{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7JcpEGxXIwi"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SaDvdkcNoKI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrgq5KKOXN-U",
        "outputId": "e3f2203e-7238-44dc-816a-7a73a4e070f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "SiHk-T6kXsuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input = pd.read_csv(\"/content/X_train_6GWGSxz.csv\")\n",
        "train_output = pd.read_csv(\"/content/y_train_2G60rOL.csv\")\n",
        "\n",
        "test_input = pd.read_csv(\"/content/X_test_c2uBt2s.csv\")\n"
      ],
      "metadata": {
        "id": "OjNJ9ykdXtzW",
        "outputId": "13f94827-474f-413d-844e-29893a4ea2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/X_train_6GWGSxz.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-606373793.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/X_train_6GWGSxz.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/y_train_2G60rOL.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/X_test_c2uBt2s.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/X_train_6GWGSxz.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_input['time_step'] = pd.to_datetime(train_input['time_step'])\n",
        "train_output['time_step'] = pd.to_datetime(train_output['time_step'])\n",
        "test_input['time_step'] = pd.to_datetime(test_input['time_step'])\n"
      ],
      "metadata": {
        "id": "ZLahNtFNYRWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_input['time_step'] = pd.to_datetime(train_input['time_step'])\n",
        "train_output['time_step'] = pd.to_datetime(train_output['time_step'])\n",
        "test_input['time_step'] = pd.to_datetime(test_input['time_step'])\n"
      ],
      "metadata": {
        "id": "ixEAhRNVYTu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Colonnes input : \", train_input.columns.tolist())\n",
        "print(\"Colonnes output :\", train_output.columns.tolist())\n"
      ],
      "metadata": {
        "id": "wUZGp8kkYa7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.head(120)\n",
        "\n"
      ],
      "metadata": {
        "id": "IWKTeu3RYbFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.plot(train_input['time_step'][:2000], train_input['consumption'][:2000])\n",
        "plt.title(\"Consommation totale - premiers 2000 points\")\n",
        "plt.xlabel(\"Temps\")\n",
        "plt.ylabel(\"W\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m9TClQoYYbNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_input = train_input.isna().mean() * 100\n",
        "missing_output = train_output.isna().mean() * 100\n",
        "\n",
        "print(\"Taux de NA - Input (%):\\n\", missing_input)\n",
        "print(\"\\nTaux de NA - Output (%):\\n\", missing_output)\n"
      ],
      "metadata": {
        "id": "TrU-ClqlYbRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3J5HHQKgmKR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteo_cols = ['visibility','temperature','humidity','humidex','windchill','wind','pressure']\n",
        "\n",
        "# Ensuite forward-fill pour remplir les minutes entre heures\n",
        "train_input[meteo_cols] = train_input[meteo_cols].bfill()\n",
        "# Ensuite forward-fill pour remplir les minutes entre heures\n",
        "train_input[meteo_cols] = train_input[meteo_cols].ffill()\n",
        "\n"
      ],
      "metadata": {
        "id": "petim45SdBnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_output[['washing_machine','fridge_freezer','TV','kettle']] = \\\n",
        "    train_output[['washing_machine','fridge_freezer','TV','kettle']].ffill().bfill()\n",
        "train_input['consumption'] = train_input['consumption'].ffill().bfill()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "theAaRGGeVjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Eq3V8KRxreSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = train_input.merge(train_output, on='time_step')\n",
        "df.head()\n",
        "df.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "Nx8wKPWXmLCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================\n",
        "\n",
        "# Extraire informations temporelles\n",
        "df['weekday'] = df['time_step'].dt.weekday   # 0 = lundi\n",
        "df['hour'] = df['time_step'].dt.hour\n",
        "df['minute'] = df['time_step'].dt.minute\n",
        "\n",
        "# Weekend (0/1)\n",
        "df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "# Encodage circulaire de l'heure\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "\n",
        "# Dummies pour jour de la semaine\n",
        "df = pd.get_dummies(df, columns=['weekday'], prefix='day')\n",
        "\n",
        "# ============================\n",
        "# 5. SÉPARATION X et Y\n",
        "# ============================\n",
        "\n",
        "# Sorties\n",
        "Y = df[['washing_machine','fridge_freezer','TV','kettle']]\n",
        "\n",
        "# Entrées\n",
        "X = df[['consumption','visibility','temperature','humidity',\n",
        "        'humidex','windchill','wind','pressure',\n",
        "        'is_weekend','hour_sin','hour_cos']\n",
        "      + [col for col in df.columns if col.startswith(\"day_\")]]\n",
        "\n",
        "print(\"X shape =\", X.shape)\n",
        "print(\"Y shape =\", Y.shape)\n"
      ],
      "metadata": {
        "id": "U7Pz7vP1mPbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Diviser en 80% pour l'entraînement et 20% pour la validation\n",
        "X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Liste des appareils\n",
        "devices = ['washing_machine', 'fridge_freezer', 'TV', 'kettle']\n",
        "\n",
        "# Dictionnaires pour stocker MAE et RMSE\n",
        "mae_dict = {}\n",
        "rmse_dict = {}\n",
        "\n",
        "# Entraîner un modèle linéaire pour chaque appareil\n",
        "for device in devices:\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Entraîner le modèle sur les données d'entraînement\n",
        "    model.fit(X_train_split, Y_train_split[device])\n",
        "\n",
        "    # Prédire les valeurs sur les données d'entraînement (ou de validation si tu veux évaluer pendant l'entraînement)\n",
        "    Y_pred = model.predict(X_val_split)\n",
        "\n",
        "    # Calculer le MAE (Mean Absolute Error)\n",
        "    mae_dict[device] = mean_absolute_error(Y_val_split[device], Y_pred)\n",
        "\n",
        "    # Calculer le RMSE (Root Mean Squared Error)\n",
        "    rmse_dict[device] = np.sqrt(mean_squared_error(Y_val_split[device], Y_pred))\n",
        "\n",
        "    # Afficher les résultats pour chaque appareil\n",
        "    print(f\"{device} MAE: {mae_dict[device]:.2f} W\")\n",
        "    print(f\"{device} RMSE: {rmse_dict[device]:.2f} W\")\n",
        "\n"
      ],
      "metadata": {
        "id": "v8uUBG0QoLN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle de régression linéaire pour la prédiction des consommations des appareils est performant pour certains appareils comme la machine à laver et la TV, avec un MAE inférieur au benchmark de 47.64 W. Cependant, des erreurs sont observées pour le frigo et la bouilloire, principalement en raison de leur comportement cyclique et impulsif. Ces limitations suggèrent que des modèles plus complexes, comme les réseaux neuronaux (LSTM, CNN), pourraient améliorer la précision pour ces appareils."
      ],
      "metadata": {
        "id": "UWp_4EPDp9X9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4womQtdKp_of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark = 47.64\n",
        "\n",
        "for device in devices:\n",
        "    print(f\"{device} - Benchmark MAE: {benchmark:.2f} W\")\n",
        "    print(f\"{device} - MAE: {mae_dict[device]:.2f} W (Better than Benchmark? {mae_dict[device] < benchmark})\\n\")\n"
      ],
      "metadata": {
        "id": "vwjdJ7ZwpCHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalisation des données\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normaliser X_train et X_val entre 0 et 1 (sauf time_step)\n",
        "X_train_scaled = scaler.fit_transform(X_train_split)\n",
        "X_val_scaled = scaler.transform(X_val_split)\n",
        "\n",
        "\n",
        "def create_lstm_dataset(X, Y, timesteps=50):\n",
        "    X_lstm, Y_lstm = [], []\n",
        "\n",
        "    for i in range(timesteps, len(X)):  # Commence à \"timesteps\" pour éviter les indices négatifs\n",
        "        X_lstm.append(X[i-timesteps:i])  # Crée une séquence de `timesteps` observations\n",
        "        Y_lstm.append(Y[i])  # La cible à prédire (la valeur à prédire à chaque `timesteps`)\n",
        "\n",
        "    return np.array(X_lstm), np.array(Y_lstm)\n",
        "\n",
        "# Créer les datasets pour LSTM\n",
        "X_train_lstm, Y_train_lstm = create_lstm_dataset(X_train_scaled, Y_train_split)\n",
        "X_val_lstm, Y_val_lstm = create_lstm_dataset(X_val_scaled, Y_val_split)\n",
        "\n",
        "# Vérifier les formes des données\n",
        "print(f\"X_train_lstm shape: {X_train_lstm.shape}\")\n",
        "print(f\"Y_train_lstm shape: {Y_train_lstm.shape}\")\n",
        "print(f\"X_val_lstm shape: {X_val_lstm.shape}\")\n",
        "print(f\"Y_val_lstm shape: {Y_val_lstm.shape}\")\n"
      ],
      "metadata": {
        "id": "Pf0uXyl_tC7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Shape of Y: {Y_train_split.shape}\")  # Taille des labels\n",
        "\n",
        "# Affichons les premiers éléments de X et Y pour s'assurer que tout est OK\n",
        "\n",
        "print(\"Premiers éléments de Y:\", Y_train_split[:100])\n"
      ],
      "metadata": {
        "id": "PMxYWl5HwN_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "964eSWJWqAZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMx4dZVFoSCX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}